{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering (CF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In month 1, we learn about some commom techniques to recommend items to a user.  \n",
    "\n",
    "\n",
    "[The 1st notebook](https://github.com/caiomiyashiro/RecommenderSystemsNotebooks/blob/master/Month%201%20Part%20I%20-%20Non%20Personalised%20and%20Stereotyped%20Recommendation.ipynb) presented non-personalised and stereotyped recommendations, which only took averages from the population's avaliations (ratings) in order to predict and present the most popular items.\n",
    "\n",
    "\n",
    "[The 2nd notebook](https://github.com/caiomiyashiro/RecommenderSystemsNotebooks/blob/master/Month%201%20Part%20III%20-%20Content%20Based%20Recommendation.ipynb) introduced a little of personalisation, where we created a user's taste vector and used it to 'match' the user array with other documents.\n",
    "    \n",
    "This notebook introduce the concept of **collaborative filtering**, a recommendation strategy to find and match similar entities. I say entities because we have two different variants on collaborative filtering: \n",
    "\n",
    "\n",
    "* User User CF: First CF technique created, the User User CF only takes into consideration only the user's past behaviour, *i.e.*, its ratings, and nothing about the items's characteristics. The ideia is pretty simple: If two users $U_{1}$ and $U_{2}$ have liked items $I_{a}$ and $I_{b}$, but user $U_{2}$ liked an item $I_{c}$ that $U_{1}$ hasn't seen yet. We infer that item $I_{c}$ would be a good recommendation for $U_{1}$. The following picture gives a good representation about it.\n",
    "\n",
    "<img src=\"images/notebook4_image1.png\" width=\"600\">\n",
    "\n",
    "* Item Item CF: The User User CF has some drawbacks, which we are going to talk about later. Because of these drawbacks, a more efficient approach was created, the Item Item CF. This technique doesn't take into consideration the users' similarities but only on item similarities. With this, new item predictions for a user $U$ can be easily calculated taking into account the ratings the user gave for similar items. This approach is going to be presented in the next notebook.\n",
    "\n",
    "# Example Dataset\n",
    "\n",
    "For the next explanations in Nearest Neighboors for CF we're going to use the [dataset](https://drive.google.com/file/d/0BxANCLmMqAyIQ0ZWSy1KNUI4RWc/view?usp=sharing) provided from the Coursera Specialisation in Recommender Systems, specifically the data from the assignment on User User CF in [course 2](https://www.coursera.org/learn/collaborative-filtering) from the specialisation: \n",
    "\n",
    "The dataset is a matrix with size 100 movies x 25 users and each cell $c_{m,u}$ contains the rating user $u$ gave to movie $m$. If user $u$ didn't rate movie $m$, the cell is empty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1648</th>\n",
       "      <th>5136</th>\n",
       "      <th>918</th>\n",
       "      <th>2824</th>\n",
       "      <th>3867</th>\n",
       "      <th>860</th>\n",
       "      <th>3712</th>\n",
       "      <th>2968</th>\n",
       "      <th>3525</th>\n",
       "      <th>4323</th>\n",
       "      <th>...</th>\n",
       "      <th>3556</th>\n",
       "      <th>5261</th>\n",
       "      <th>2492</th>\n",
       "      <th>5062</th>\n",
       "      <th>2486</th>\n",
       "      <th>4942</th>\n",
       "      <th>2267</th>\n",
       "      <th>4809</th>\n",
       "      <th>3853</th>\n",
       "      <th>2288</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11: Star Wars: Episode IV - A New Hope (1977)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4,5</td>\n",
       "      <td>5</td>\n",
       "      <td>4,5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4,5</td>\n",
       "      <td>4</td>\n",
       "      <td>3,5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12: Finding Nemo (2003)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4,5</td>\n",
       "      <td>4,5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3,5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3,5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13: Forrest Gump (1994)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>4,5</td>\n",
       "      <td>5</td>\n",
       "      <td>4,5</td>\n",
       "      <td>4,5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>4,5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3,5</td>\n",
       "      <td>4,5</td>\n",
       "      <td>4,5</td>\n",
       "      <td>4</td>\n",
       "      <td>3,5</td>\n",
       "      <td>4,5</td>\n",
       "      <td>3,5</td>\n",
       "      <td>3,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14: American Beauty (1999)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4,5</td>\n",
       "      <td>2</td>\n",
       "      <td>3,5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3,5</td>\n",
       "      <td>4,5</td>\n",
       "      <td>3,5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3,5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22: Pirates of the Caribbean: The Curse of the Black Pearl (2003)</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4,5</td>\n",
       "      <td>4</td>\n",
       "      <td>2,5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1,5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2,5</td>\n",
       "      <td>3,5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3,5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   1648 5136  918 2824 3867  \\\n",
       "11: Star Wars: Episode IV - A New Hope (1977)       NaN  4,5    5  4,5    4   \n",
       "12: Finding Nemo (2003)                             NaN    5    5  NaN    4   \n",
       "13: Forrest Gump (1994)                             NaN    5  4,5    5  4,5   \n",
       "14: American Beauty (1999)                          NaN    4  NaN  NaN  NaN   \n",
       "22: Pirates of the Caribbean: The Curse of the ...    4    5    3  4,5    4   \n",
       "\n",
       "                                                    860 3712 2968 3525 4323  \\\n",
       "11: Star Wars: Episode IV - A New Hope (1977)         4  NaN    5    4    5   \n",
       "12: Finding Nemo (2003)                               4  4,5  4,5    4    5   \n",
       "13: Forrest Gump (1994)                             4,5  NaN    5  4,5    5   \n",
       "14: American Beauty (1999)                          NaN  4,5    2  3,5    5   \n",
       "22: Pirates of the Caribbean: The Curse of the ...  2,5  NaN    5    3    4   \n",
       "\n",
       "                                                   ...  3556 5261 2492 5062  \\\n",
       "11: Star Wars: Episode IV - A New Hope (1977)      ...     4  NaN  4,5    4   \n",
       "12: Finding Nemo (2003)                            ...     4  NaN  3,5    4   \n",
       "13: Forrest Gump (1994)                            ...     4    5  3,5  4,5   \n",
       "14: American Beauty (1999)                         ...     4  NaN  3,5  4,5   \n",
       "22: Pirates of the Caribbean: The Curse of the ... ...     3  1,5    4    4   \n",
       "\n",
       "                                                    2486 4942 2267 4809 3853  \\\n",
       "11: Star Wars: Episode IV - A New Hope (1977)        3,5  NaN  NaN  NaN  NaN   \n",
       "12: Finding Nemo (2003)                                2  3,5  NaN  NaN  NaN   \n",
       "13: Forrest Gump (1994)                              4,5    4  3,5  4,5  3,5   \n",
       "14: American Beauty (1999)                           3,5    4  NaN  3,5  NaN   \n",
       "22: Pirates of the Caribbean: The Curse of the ...   2,5  3,5  NaN    5  NaN   \n",
       "\n",
       "                                                   2288  \n",
       "11: Star Wars: Episode IV - A New Hope (1977)       NaN  \n",
       "12: Finding Nemo (2003)                             3,5  \n",
       "13: Forrest Gump (1994)                             3,5  \n",
       "14: American Beauty (1999)                          NaN  \n",
       "22: Pirates of the Caribbean: The Curse of the ...  3,5  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/User-User Collaborative Filtering - movie-row.csv', index_col=0)\n",
    "print('Dataset shape: ' + str(df.shape))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighboors for CF\n",
    "\n",
    "The approach for doing CF with nearest neighboors is to compare what you want to be matched with other similiar entities. With this, we have to define two things: \n",
    "  \n",
    "* One, in order to bring the most similar items or other customers with similar tastes, we must limit the amount of entities we compare it with.\n",
    "* Second, when doing predictions for an unseen data, we must match it with neighboors who have already rated the data we want.\n",
    "  \n",
    "With these two constraints, we see we have a trade off when deciding the amount of neighboors. If the number of neighboors is set to a too low value, the chances is that we end up with a lot of entities not having reviewed the same thing, and we end up not being able to provide confident predictions for our objective. If we set the bar too high, we will include too many different neighboors in our comparison, with different tastes than the user we want predict recommendations to.\n",
    "\n",
    "(**reference**) made a feel experiments with different configurations for User User CF and discovered that, for most commercial applications used nowadays, an optimal number of neighboors to consider is between 20 and 30. \n",
    "\n",
    "## Similarity Function\n",
    "\n",
    "The next step to define what are going to be the neighboors of a specific user is to define the similarity metric. In the User User CF context, the input data is a matrix where the rows are the users, columns are the items, and each cell $C_{u,i}$ is the rating that user $u$ gave to item $i$. So, if we want to compare the similarity in terms of ratings between two users $u_{1}$ and $u_{2}$, we have as input to the similarty function, two arrays, containing all reviews that each user made to each item, and blank values when the user didn't rate that specific item.  \n",
    "\n",
    "(**reference**) made a few experiments with similarity metric and pointed that, in the context of User User CF, the pearson correlation performed well in terms of finding good user neighboors to get data for predictions.\n",
    "\n",
    "The person correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on the Pearson Correlation Coefficient\n",
    "\n",
    "The pearson correlation coefficient comes from the covariance factor between two variables normalised to have a bounded value between 0 and 1 and answers the following question: **How much linear correlated** the variables $x$ and $y$ are? As the values are normalised, its possible to have some guidelines for the coefficient value, such as:\n",
    "\n",
    "* Exactly 1. A perfect uphill (positive) linear relationship\n",
    "* 0.70. A strong uphill (positive) linear relationship\n",
    "* 0.50. A moderate uphill (positive) linear relationship\n",
    "* 0.30. A weak uphill (positive) linear relationship\n",
    "* 0 no **linear** relationship, neither positive nor negative\n",
    "* The same for negative values\n",
    "\n",
    "The value comes from dividing the covariance between variables $x$ and $y$ and dividing it by the product of $x$ and $y$ standard deviations:\n",
    "\n",
    "$$r_{x,y} = \\frac{S_{xy}}{S_{x}S_{y}}\\hspace{7.0cm}$$\n",
    "\n",
    "$$= \\frac{\\frac{\\sum_{i=1}^{n}(x_{i} - \\bar{x})(y_{i} - \\bar{y})}{n-1}}{\\frac{\\sqrt{\\sum_{i=1}^{n}(x_{i} - \\bar{x})^{2}}}{n-1}  \\frac{\\sqrt{\\sum_{i=1}^{n}(y_{i} - \\bar{y}})^{2}}{n-1}}\\hspace{4.5cm}(1)$$\n",
    "\n",
    "$$= \\frac{\\sum_{i=1}^{n}(x_{i} - \\bar{x})(y_{i} - \\bar{y})}  {\\sqrt{\\sum_{i=1}^{n}(x_{i} - \\bar{x})^{2}} \\sqrt{\\sum_{i=1}^{n}(y_{i} - \\bar{y})^{2}}}\\hspace{3cm}(2)$$\n",
    "\n",
    "Lets take a look first at equation (1):\n",
    "\n",
    "- Nominator: It basically extracts the following info:\n",
    "    - For a covariance value of $c$. In average, a dislocation in 1 unit from the mean value in $x$ represent a dislocation in $c^{2}$ units in $y$.\n",
    "    - Differently from the 1 variable standard deviation = square root of the variance, we don't take square roots from the covariance values, as it is still not easily understandable. For example, the covariance output would be in the unit $x^{2}y^{2}$ and taking the square root would still be on the unit $xy$. Instead of finding a a proper valid unit transformation, people tend to go directly to a unitless variable, the correlation.\n",
    "      \n",
    "      \n",
    "      \n",
    "- Denominator: Normalisation factor to transform the output between -1 and 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between Pearson and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
